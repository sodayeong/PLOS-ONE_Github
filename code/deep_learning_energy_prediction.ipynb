{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55aef6aa",
   "metadata": {},
   "source": [
    "# Deep Learning and Hybrid Models for Electricity Consumption Prediction\n",
    "\n",
    "In this notebook, we implement various deep learning models for residential building electricity consumption forecasting based on the provided datasets.<br>The models considered include LSTM, Bi-LSTM, GRU, Bi-GRU, 1D-CNN, and TCN.<br>In addition, we explore hybrid models such as LSTM-TCN, BiLSTM-TCN, GRU-TCN, and BiGRU-TCN.\n",
    "\n",
    "The hyperparameters are set as per your specifications:\n",
    "* Optimizer: Adam\n",
    "* Learning rate: 0.001\n",
    "* Attention mechanism: True\n",
    "* Activation function: LeakyReLU\n",
    "* Random state: 42\n",
    "* Batch size: 24\n",
    "* Epochs: 100\n",
    "\n",
    "The evaluation metrics used are as follows:\n",
    "* Mean absolute percentage error (MAPE);\n",
    "* Coefficient of variation of the root mean squared error (CVRMSE);\n",
    "* Normalized mean absolute error (NMAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install keras-tcn keras-self-attention\n",
    "\n",
    "# Import essential libraries\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Import deep learning libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (TimeDistributed, Bidirectional, GRU, LSTM,\n",
    "                                     Conv1D, Flatten, Dense, Concatenate, RepeatVector,\n",
    "                                     GlobalAveragePooling1D, MaxPooling1D, Reshape, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tcn import TCN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98df05",
   "metadata": {},
   "source": [
    "## Load and Split the Datasets\n",
    "\n",
    "We use the provided `load_and_split_data' function to load either the 'household' or 'dormitory' dataset.<br>The data are split into training and test sets based on predefined indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79822d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(dataset_choice):\n",
    "    if dataset_choice == 'household':\n",
    "        data = pd.read_csv('Appliances Energy Prediction.csv')\n",
    "        X_train = data.iloc[:2311, 1:-1]\n",
    "        y_train = data.iloc[:2311, -1]\n",
    "        X_test = data.iloc[2311:, 1:-1]\n",
    "        y_test = data.iloc[2311:, -1]\n",
    "    elif dataset_choice == 'dormitory':\n",
    "        data = pd.read_csv('University Residential Complex.csv')\n",
    "        X_train = data.iloc[:20472, 5:-1]\n",
    "        y_train = data.iloc[:20472, -1]\n",
    "        X_test = data.iloc[20472:, 5:-1]\n",
    "        y_test = data.iloc[20472:, -1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset choice. Please select 'household' or 'dormitory'.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Select the dataset\n",
    "dataset_choice = 'household'  # Change to 'dormitory' as needed\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = load_and_split_data(dataset_choice)\n",
    "\n",
    "# Display shapes\n",
    "print(\"Training set shape:\", X_train_raw.shape, y_train_raw.shape)\n",
    "print(\"Test set shape:\", X_test_raw.shape, y_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070778",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We scale the features using `MinMaxScaler` to standardize the data.<br>We then prepare the data for time series forecasting by creating sequences with `n_past` time steps to predict `n_future` time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test_raw.values.reshape(-1, 1))\n",
    "\n",
    "# Define past and future time steps\n",
    "n_past = 24   # Number of past time steps\n",
    "n_future = 24  # Number of future time steps to predict\n",
    "y_feature = 1  # Since we're predicting one feature (energy consumption)\n",
    "\n",
    "# Prepare data for time series forecasting\n",
    "def create_sequences(X, y, n_past, n_future):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(n_past, len(X) - n_future +1):\n",
    "        Xs.append(X[i - n_past:i])\n",
    "        ys.append(y[i:i + n_future])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Create sequences for training and testing\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, n_past, n_future)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, n_past, n_future)\n",
    "\n",
    "print(\"Training sequences shape:\", X_train_seq.shape, y_train_seq.shape)\n",
    "print(\"Testing sequences shape:\", X_test_seq.shape, y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f32c3",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics\n",
    "\n",
    "We define functions for MAPE, CVRMSE, and NMAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def CVRMSE(y_true, y_pred):\n",
    "    return (np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.mean(y_true)) * 100\n",
    "\n",
    "def NMAE(y_true, y_pred):\n",
    "    return (np.mean(np.abs(y_true - y_pred)) / np.mean(y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7703693",
   "metadata": {},
   "source": [
    "## Build and Train Models\n",
    "\n",
    "We define a function to build and train various deep learning models based on the selected architecture.<br>The models include LSTM, Bi-LSTM, GRU, Bi-GRU, 1D-CNN, TCN, and hybrid models such as LSTM-TCN, BiLSTM-TCN, GRU-TCN, and BiGRU-TCN.<br>We use the LeakyReLU activation function and standardize the hyperparameters as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ac787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model_select, attention, activation):\n",
    "    encoder_inputs = Input(shape=(n_past, X_train_seq.shape[2]))\n",
    "    y_feature = 1  # Number of features to predict\n",
    "\n",
    "    if model_select == 'lstm-tcn':\n",
    "        # LSTM-TCN Hybrid Model\n",
    "        encoder_lstm = LSTM(14, return_state=True, activation='linear')\n",
    "        encoder_outputs1 = encoder_lstm(encoder_inputs)\n",
    "        encoder_states1 = encoder_outputs1[1:]  # Hidden and cell states\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_tcn = TCN(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_tcn = LeakyReLU()(decoder_tcn)\n",
    "        decoder_l1 = LSTM(14, return_sequences=True, activation='linear')(decoder_tcn, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "\n",
    "    elif model_select == 'bilstm-tcn':\n",
    "        # BiLSTM-TCN Hybrid Model\n",
    "        encoder_bilstm = Bidirectional(LSTM(14, return_state=True, activation='linear'))\n",
    "        encoder_outputs1 = encoder_bilstm(encoder_inputs)\n",
    "        encoder_states1 = encoder_outputs1[1:]\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_tcn = TCN(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_tcn = LeakyReLU()(decoder_tcn)\n",
    "        decoder_l1 = Bidirectional(LSTM(14, return_sequences=True, activation='linear'))(decoder_tcn, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "\n",
    "    elif model_select == 'gru-tcn':\n",
    "        # GRU-TCN Hybrid Model\n",
    "        encoder_gru = GRU(14, return_state=True, activation='linear')\n",
    "        encoder_outputs1 = encoder_gru(encoder_inputs)\n",
    "        encoder_states1 = [encoder_outputs1[1]]  # Hidden state\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_tcn = TCN(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_tcn = LeakyReLU()(decoder_tcn)\n",
    "        decoder_l1 = GRU(14, return_sequences=True, activation='linear')(decoder_tcn, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "\n",
    "    elif model_select == 'bigru-tcn':\n",
    "        # BiGRU-TCN Hybrid Model\n",
    "        encoder_bigru = Bidirectional(GRU(14, return_state=True, activation='linear'))\n",
    "        encoder_outputs1 = encoder_bigru(encoder_inputs)\n",
    "        encoder_states1 = [encoder_outputs1[1]]  # Hidden state\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_tcn = TCN(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_tcn = LeakyReLU()(decoder_tcn)\n",
    "        decoder_l1 = Bidirectional(GRU(14, return_sequences=True, activation='linear'))(decoder_tcn, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == 'lstm':\n",
    "        # LSTM Single Model\n",
    "        encoder_lstm = LSTM(14, return_state=True, activation='linear')\n",
    "        encoder_outputs1 = encoder_lstm(encoder_inputs)\n",
    "        encoder_states1 = encoder_outputs1[1:]\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_l1 = LSTM(14, return_sequences=True, activation='linear')(decoder_inputs, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == 'bilstm':\n",
    "        # BiLSTM Single Model\n",
    "        encoder_bilstm = Bidirectional(LSTM(14, return_state=True, activation='linear'))\n",
    "        encoder_outputs1 = encoder_bilstm(encoder_inputs)\n",
    "        encoder_states1 = encoder_outputs1[1:]\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_l1 = Bidirectional(LSTM(14, return_sequences=True, activation='linear'))(decoder_inputs, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == 'gru':\n",
    "        # GRU Single Model\n",
    "        encoder_gru = GRU(14, return_state=True, activation='linear')\n",
    "        encoder_outputs1 = encoder_gru(encoder_inputs)\n",
    "        encoder_states1 = [encoder_outputs1[1]]\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_l1 = GRU(14, return_sequences=True, activation='linear')(decoder_inputs, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == 'bigru':\n",
    "        # BiGRU Single Model\n",
    "        encoder_bigru = Bidirectional(GRU(14, return_state=True, activation='linear'))\n",
    "        encoder_outputs1 = encoder_bigru(encoder_inputs)\n",
    "        encoder_states1 = [encoder_outputs1[1]]\n",
    "        encoder_outputs = LeakyReLU()(encoder_outputs1[0])\n",
    "        decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "        decoder_l1 = Bidirectional(GRU(14, return_sequences=True, activation='linear'))(decoder_inputs, initial_state=encoder_states1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == '1d-cnn':\n",
    "        # 1D CNN Model\n",
    "        conv1d = Conv1D(filters=14, kernel_size=2, activation='linear')(encoder_inputs)\n",
    "        conv1d = LeakyReLU()(conv1d)\n",
    "        flatten = Flatten()(conv1d)\n",
    "        decoder_inputs = RepeatVector(n_future)(flatten)\n",
    "        decoder_l1 = LSTM(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    elif model_select == 'tcn':\n",
    "        # TCN Model\n",
    "        tcn_layer = TCN(14, return_sequences=False, activation='linear')(encoder_inputs)\n",
    "        tcn_layer = LeakyReLU()(tcn_layer)\n",
    "        decoder_inputs = RepeatVector(n_future)(tcn_layer)\n",
    "        decoder_l1 = TCN(14, return_sequences=True, activation='linear')(decoder_inputs)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model selection.\")\n",
    "\n",
    "    # Attention mechanism\n",
    "    if attention:\n",
    "        decoder_l1 = SeqSelfAttention(attention_activation='linear')(decoder_l1)\n",
    "        decoder_l1 = LeakyReLU()(decoder_l1)\n",
    "    \n",
    "    decoder_outputs1 = TimeDistributed(Dense(y_feature, activation='linear'))(decoder_l1)\n",
    "    model_result = Model(encoder_inputs, decoder_outputs1)\n",
    "    model_result.summary()\n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6913ce6",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "\n",
    "Each model will be trained for 100 epochs with a batch size of 24, aligning with the daily power consumption cycle.<br>We use the `EarlyStopping` callback to prevent overfitting.<br>A random state of 42 ensures reproducibility.\n",
    "\n",
    "The hyperparameters are standardized across experiments as specified:\n",
    "- Optimizer: Adam with a learning rate of 0.001\n",
    "- Activation function: LeakyReLU\n",
    "- Attention mechanism: True\n",
    "- Batch size: 24\n",
    "- Epochs: 100\n",
    "- Random state: 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Prepare callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# List of models to train\n",
    "model_list = ['lstm', 'bilstm', 'gru', 'bigru', '1d-cnn', 'tcn', 'lstm-tcn', 'bilstm-tcn', 'gru-tcn', 'bigru-tcn']\n",
    "\n",
    "# Dictionary to store trained models\n",
    "results = {}\n",
    "\n",
    "# Training and evaluation loop\n",
    "for model_name in model_list:\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    # Build the model\n",
    "    model = model_training(model_select=model_name, attention=True, activation='leaky_relu')\n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=24,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop, tensorboard_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    # Save the trained model\n",
    "    results[model_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce4e34",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "We evaluate each model using the defined metrics and compare their performance.<br>We also save the evaluation metrics in a CSV file named `deep_learning_evaluation_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64089bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store evaluation results\n",
    "evaluation_results = []\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    model = results[model_name]\n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    # Reshape predictions and true values\n",
    "    y_pred_scaled = y_pred_scaled.reshape(-1, 1)\n",
    "    y_test_scaled_flat = y_test_seq.reshape(-1, 1)\n",
    "    # Inverse transform to get actual values\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    y_true = scaler_y.inverse_transform(y_test_scaled_flat)\n",
    "    # Calculate evaluation metrics\n",
    "    mape = MAPE(y_true, y_pred)\n",
    "    cvrmse = CVRMSE(y_true, y_pred)\n",
    "    nmae = NMAE(y_true, y_pred)\n",
    "    # Print evaluation results\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"CVRMSE: {cvrmse:.2f}%\")\n",
    "    print(f\"NMAE: {nmae:.2f}%\")\n",
    "    # Append results to evaluation_results\n",
    "    evaluation_results.append({\n",
    "        'Model': model_name,\n",
    "        'MAPE': mape,\n",
    "        'CVRMSE': cvrmse,\n",
    "        'NMAE': nmae\n",
    "    })\n",
    "\n",
    "# Save evaluation metrics to a .csv file\n",
    "csv_file_name = \"deep_learning_evaluation_metrics.csv\"\n",
    "with open(csv_file_name, \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"Model\", \"MAPE\", \"CVRMSE\", \"NMAE\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in evaluation_results:\n",
    "        writer.writerow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec27936",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We trained and evaluated several deep learning models, including single and hybrid models, for residential building electricity consumption forecasting.<br>The models were implemented exactly as specified in the provided code and trained with standardized hyperparameters to ensure a fair comparison.<br>The evaluation metrics indicate the performance of the models and we can select the best performing model based on these results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
